\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}
\usepackage[margin=1in]{geometry}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\set}[1]{\left\{ #1 \right\}}

\title{Applications of the EM Algorithm}
\author{Ryan Cooper}

\begin{document}
\maketitle

\section*{Introduction}
The Expectation Maximization (EM) algorithim is a useful algorithm for maximizing a likelihood function when latent variables are present. The essential idea behind the algorithim is to iteratively compute the expected value of the latent variables given the data and current parameter estimates and then use this conditional expectation in the likelihood function and update the maximum likelihood estimates.  For a full discription of the algorithm and its important properties see [1]. The aim of this report is to use the EM algorithm to find maximum likelihood estimates for parameters in models with incomplete data. In the first section we analyze continuous univariate data and fit a two-component normal model. Additionaly, we use the bootstrap to test model assumptions and provide confidence intervals for the median. For the second section we fit both a one and two-component Poisson model to count data and test the goodness of fit for each model. 

\section{Two-Component Gaussian Mixture}
<<echo=F>>=
require(knitr)
library(xtable)
options(xtable.floating = T)
options(xtable.timestamp = "")
opts_chunk$set(echo=F, fig.align='center')

# Read in the data
d1 = read.csv("d1.csv"); x = d1$z

# Shapiro-Wilk test for normality
swtest = shapiro.test(x)
@

The data in this section consists of 90 observations from a continuous random variable. Let $X' = (X_1,\ldots,X_{90})$ denote the data for this section. The histogram of the sample $X$ shown in Figure 1 indicates that the distribution is bi-modal and the normal assumption is not appropriate. After testing the fit of $X$ against a single normal distribution, we fit a two component Gaussian mixture model using the EM algorithm.

\subsection*{Testing for Normality}
A natural test in this composite null hypothesis setting is the Shapiro-Wilk test for normality, which is known to be sensitive to a wide range of alternatives. The test statistic is of anova type; it's proportional to the ratio of two estimators for the population variance. For a description of the test statistic and its properties see [2]. The resulting $p$-value for the test is \Sexpr{round(swtest$p.value,4)}. Hence we reject the null hypothesis that the sample was drawn from a single normal distribution at the 0.05 level. Based on the histogram in Figure 1, a more appropriate model for the data is a two-component Gaussian mixture.


<<EMnormal>>=
# EM Algorithm for Normal Mixture

## Conditional expectation used in EM algorithim for Normal Mixture
CENM = function(x,p,m0,m1,v0,v1){
  mar = p*dnorm(x,m1,sqrt(v1)) + (1-p)*dnorm(x,m0,sqrt(v0))
  p*dnorm(x,m1,sqrt(v1))/mar
}

## EM algorithim
EMNM = function(x,p,m0,m1,v,eqvar){
  #Parameters: 
  # x = univariate data
  # p = Initial mixing parameter
  # m0, m1 = Initial means for group 0, group 1
  # v = Initial variance for both group 0 and group 1
  # eqvar = TRUE if v0=v1 in model assumption
  
  # Returns EM estimate for p, m0, m1, v0, v1
  
  Tol = 0.0009 # Tolerance level
  n = length(x)
  done = F
  
  if(eqvar == T){
    nIter = 1
    results = matrix(c(nIter, p, m0, m1, v), nrow = 1, ncol = 5)
    colnames(results) = c("Iter", "$p$", "$\\mu_0$",
                          "$\\mu_1$", "$\\sigma^2$")
    while(done == F){
      w = CENM(x,p,m0,m1,v,v)
      p.new = mean(w)
      m0.new = sum((1-w)*x)/(n - sum(w))
      m1.new = sum(w*x)/sum(w)
      v.new = sum(w*(x-m1.new)^2 + (1-w)*(x-m0.new))/n
    
      diff = abs(c(p.new - p, m0.new- m0, m1.new - m1, v.new - v))
      if(max(diff) <= Tol) done = T
      p = p.new; m0 = m0.new; m1 = m1.new; v = v.new
      nIter = nIter + 1
      
      results = rbind(results, c(nIter,p,m0,m1,v))
    }
  }
  
  else{
    v0 = v1 = v
    nIter = 1
    results = matrix(c(nIter,p, m0, m1, v0, v1), nrow = 1, ncol = 6)
    colnames(results) = c("Iter", "$p$", "$\\mu_0$", "$\\mu_1$",
                          "$\\sigma_0^2$", "$\\sigma_1^2$")
    while(done == F){
      w = CENM(x,p,m0,m1,v0,v1)
      p.new = mean(w)
      m0.new = sum((1-w)*x)/(n - sum(w))
      m1.new = sum(w*x)/sum(w)
      v0.new = sum((1-w)*(x - m0)^2)/(n-sum(w))
      v1.new = sum(w*(x-m1)^2)/sum(w)
    
      diff = abs(c(p.new - p, m0.new - m0, m1.new - m1,
                   v0.new - v0, v1.new - v1))
      if(max(diff) <= Tol) done = T
      p = p.new; m0 = m0.new; m1 = m1.new; v0 = v0.new; 
      v1 = v1.new; nIter = nIter + 1
      results = rbind(results,c(nIter,p,m0,m1,v0,v1))
    }
  }
  results
}

p = 1/2; m0 = quantile(x, 0.33); m1 = quantile(x,0.67); v=IQR(x)^2

EM1 = EMNM(x,p,m0,m1,v,T)
EM2 = EMNM(x,p,m0,m1,v,F)
@

\subsection*{Fitting a Two-Component Normal Mixture via EM}
Now consider two possible models for the observation $X$. Suppose that $X_1,\ldots,X_{90}$ is an iid sample from
\[
  (1-p)N(\mu_0, \sigma_0^2) + p N(\mu_1, \sigma_1^2).
\]
The first model assumes that $\sigma_0 = \sigma_1$ while the second assumes no restriction on $\sigma_0$ and $\sigma_1$. For each model we use the EM algorithm to obtain the maximum likelihood estimates for the model parameters. Tables 1 and 2 show the iterations up to three decimals of accuracy of the EM algorithm applied to each model.

<<EM1output, results = 'asis'>>=
# Model 1: v0 = v1
r1 = nrow(EM1)
s1 = append(seq(0,r1,5), r1); s1[1]=1 #Choose iterations to display
Iter1 = round(EM1[s1,], 4)
caption1 = paste("Iterations of the EM algorithm for fitting a",
                 "two-component normal mixture with",
                 "$\\sigma_0 = \\sigma_1 = \\sigma$")
print(xtable(Iter1, auto=T, label="EM1", caption=caption1),
      include.rownames=F,
      sanitize.text.function = function(x){x})
@

<<EM2output, results = 'asis'>>=
# Model 2: No assumption on v0 and v1
r2 = nrow(EM2)
s2 = append(seq(0,r2,5),r2); s2[1]=1
Iter2 = round(EM2[s2,],4)
caption2 = paste("Iterations of the EM algorithm for fitting a",
                 "two-component normal mixture with",
                 "$\\sigma_0$ and $\\sigma_1$ unconstrained.")
print(xtable(Iter2, auto=T, label="EM2", caption=caption2),
      include.rownames = F,
      sanitize.text.function = function(x){x})
@

<<kstest>>=
f1 = function(x){
  (1-EM1[r1,2])*dnorm(x, mean = EM1[r1,3], sd = sqrt(EM1[r1,5])) +
      EM1[r1,2]*dnorm(x, mean = EM1[r1,4], sd = sqrt(EM1[r1,5]))
}
kstest1 = ks.test(x, f1, alternative = "two.sided")

f2 = function(x){
  (1-EM2[r2,2])*dnorm(x, mean = EM2[r2,3], sd = sqrt(EM2[r2,5])) +
    EM2[r2,2]*dnorm(x, mean = EM2[r2,4], sd = sqrt(EM2[r2,6]))
}
kstest2 = ks.test(x, f2, alternative = "two.sided")
@

Figure 1 shows the histogram of $X$ overlaid with the two density estimates. The figure indicates that the unrestricted assumption on $\sigma_0$ and $\sigma_1$ produces a better candidate for the model density of $X$. However, note that the Kolmogorov-Smirnov goodness-of-fit test gives a $p$-value of less than 0.001 for both models, so we reject each hypothesis that $X$ is an iid sample from one of the fitted models at the 0.05 level.

\begin{figure}[!ht]
<<hist, fig.width=5, fig.height=4>>=
# Histogram of x with the two EM estimated normal mixtures.
hist(x, breaks = 20, freq = F, ylim = c(0,0.27),
     main = "", xlab = "x")
pts = seq(0,max(x), by = 0.1)
lines(pts, f1(pts), col = "red")
lines(pts, f2(pts), col = "blue")
@
\caption{Histogram with density estimate for each of the normal mixture models (red: $\sigma_0 = \sigma_1$, blue: $\sigma_0$ and $\sigma_1$ unrestricted).}\label{fig:histnm}
\end{figure}

\subsection*{Testing $\bs{\sigma_0 = \sigma_1}$ vs $\bs{\sigma_0 \neq \sigma_1}$}

<<vartest>>=
set.seed(2017)
bootvar = c(); B = 1e3
for(i in 1:B){
  s = sample(x, length(x), replace = T)
  EM = EMNM(s,p,m0,m1,v,F); r = nrow(EM)
  bootvar[i] = sqrt(EM[r,5]) - sqrt(EM[r,6])
}
obs.diff = sqrt(EM2[r2,5]) - sqrt(EM2[r2,6])
alpha = 0.05
diffCI.upper = 2*(obs.diff) - quantile(bootvar, alpha/2)
diffCI.lower = 2*(obs.diff) - quantile(bootvar, 1-alpha/2)
@

In this section we test the null hypothesis that $\sigma_0 = \sigma_1$ versus $\sigma_0 \neq \sigma_1$. The test is given by using the EM alorithm to create a bootstrap 95\% confidence interval for the difference $\sigma_0 - \sigma_1$ and rejecting the null hypothesis if 0 is not in the interval. The 95\% confidence interval for $\sigma_0 - \sigma_1$ is $(\Sexpr{round(diffCI.lower,3)}, \Sexpr{round(diffCI.upper, 3)})$. Since 0 is in the interval we fail to reject the null hypothesis at level 0.05.


\subsection*{Median}

<<median>>=
obs.median = median(x)
bootmedian = c()
for(i in 1:B){
  s = sample(x, length(x), replace = T)
  bootmedian[i] = median(s)
}
# Boot confidence interval
b.medianCI = 2*obs.median-quantile(bootmedian, c(1-alpha/2, alpha/2))

# Order confidence interval
o.medianCI = sort(x)[qbinom(c(alpha/2, 1-alpha/2), length(x), 0.5)]

@

In this section we construct two 95\% confidence intervals for the median of the distribution. The sample median is \Sexpr{round(obs.median, 4)}. Our first confidence interval for the median is gotten by using the bootstrap. The pivotal bootstrap confidence interval for the median is $(\Sexpr{round(b.medianCI,3)})$. The second confidence interval is also nonparametric and is obtained by noting that
\[
  \mathbb{P}(X_{(j)} \leq \xi_q \leq X_{(k)}) = 
  \sum_{i = j}^{k-1} \binom{n}{i}q^i (1-q)^{n-i}
\]
where $X_{(j)}$ is the $j$th order statistic and $\xi_q$ is the $q$th quantile. Using this fact we obtain $(\Sexpr{round(o.medianCI,3)})$ as an approximate 95\% confidence interval for the median. The length bootstrap confidence interval is \Sexpr{round(diff(b.medianCI),3)}, while the length of the second confidence interval is \Sexpr{round(diff(o.medianCI),3)}. Hence both methods produce confidence intervals of the same approximate length. Note that the second confidence interval obtained through the order statistics is more simpler to implement and thus a more convienent interval with the same approximate coverage as the bootstrap interval. Lastly, a third possible confidence interval can be obtained by using the asympotic normality of the median. However, this method requires an estimator of the density at the median, and although we have estimated the density via the EM algorithm, it's lack of fit may result in an unreliable confidence interval.

\section{Two Component Poisson Mixture}
<<dispersion>>=
d2 = read.csv("d2.csv")
attach(d2); n = sum(freq)

# MLE for one component Poisson model
m.mle = sum(deaths*freq)/n

# Dispersion statistic
dstat = sum(freq*(deaths-m.mle)^2)/m.mle
pval.dstat = 1 - pnorm((dstat-n+1)/sqrt(2*(n-1)))
@

The data in this section comes from the London Times and is a count of the number of deaths per day amoung women 80 years and older. A total of 1096 deaths were recorded with a range of 0 to 9 deaths per day. Let $Y_i$ denote the number of deaths on day $i$ for $i = 1,\ldots,n$. It is reasonable to assume that the deaths are independent from one another, so we can model $Y_i$ with a Poisson distribution with rate $\lambda_i$.

First we test the hypothesis that the observations are a sample from a single component Poisson distribution. Formally, we are testing $H_0$: there exists $\lambda >0$ such that $\lambda_i = \lambda$ for all $i$ versus $H_1$: $\lambda_i$ are unconstrained. In this setting the likelihood ratio is approximately equal to the dispersion statistic
\[
  T = \frac{1}{\overline{Y}}\sum (Y_i - \overline{Y})^2.
\]
Under the null hypothesis $T$ is approximately chi-square distributed with $n-1$ degrees of freedom. The resulting $p$-value for the sample is $\Sexpr{pval.dstat}$. Thus we reject $H_0$ at the 0.05 level.

<<EMpoisson>>=
# p value 0.98,  p 0.288, lambda 1 2.577, lamda 0 1.11
# Conditional expectation used in EM algorithm for Poisson Mixture.
CEPM = function(x,p,m0,m1){
  mar = p*dpois(x,m1) + (1-p)*dpois(x,m0)
  p*dpois(x,m1)/mar
}

EMPM = function(x,p,m0,m1){
  #Parameters: 
  # x = vector of events
  # c = vector of corresponding event frequencies
  # p = Initial mixing parameter
  # m0, m1 = Initial means for group 0, group 1
  
  # Returns EM estimate for p, m0, m1
  
  Tol = 0.0009 # Tolerance level
  n = length(x)
  done = F
  
  nIter = 1
  results = matrix(c(nIter,p, m0, m1), nrow = 1, ncol = 4)
  colnames(results) = c("Iter", "p", "m0", "m1")
  while(done == F){
    w = CEPM(x,p,m0,m1)
    p.new = sum(w)/n
    m0.new = sum((1-w)*x)/(n-sum(w))
    m1.new = sum(w*x)/sum(w)
  
    diff = abs(c(p.new - p, m0.new - m0, m1.new - m1))
    if(max(diff) <= Tol) done = T
    p = p.new; m0 = m0.new; m1 = m1.new; nIter = nIter + 1
    results = rbind(results,c(nIter,p,m0,m1))
  }
  results
}
y = rep(deaths, freq)
EM3 = EMPM(y, 0.3, 2.5, 1)
r3 = nrow(EM3)
@

<<chisqtest>>=
# Group 7+ deaths so that expected frequency is at least 5.
obs.prop = c(freq[1:7], sum(freq[8:10]))/n

exp.prop1 = c(dpois(0:6, m.mle), ppois(6, m.mle, lower.tail = F))


exp.prop2 = c((1-EM3[r3,2])*dpois(0:6, EM3[r3,3]) 
                + EM3[r3,2]*dpois(0:6, EM3[r3,4]),
                (1-EM3[r3,2])*ppois(6,EM3[r3,3],lower.tail = F)
                + EM3[r3,2]*ppois(6,EM3[r3,4],lower.tail = F))

fit1 = chisq.test(obs.prop*n, p = exp.prop1)

fit2 = chisq.test(obs.prop*n, p = exp.prop2)
@

Next we fit a two-component Poisson mixture model to the data. Suppose $Y_1,\ldots,Y_n$ is an iid sample from
\[
  (1-p)\text{Pois}(\lambda_0) + p\text{Pois}(\lambda_1).
\]
Using the EM algorithm the maximum likelihood estimates for $p, \lambda_0$ and $\lambda_1$ are \Sexpr{round(EM3[r3,2:3],4)} and \Sexpr{round(EM3[r3,4],4)}, respectively. To test how well the distribution fits the data, we apply the chi-square goodness-of-fit test. The resulting $p$-value is \Sexpr{round(fit2$p.value,4)}, so we accept the null hypothesis that the count proportions are determined by a two-component Poisson mixture model at the 0.05 level. The table shows the frequency of deaths for the observed data, the expected one-component Poisson model and two-component Poisson model. Note that the lack of fit for the one-component Poisson model appears most prominently in 0, 2, 3 and 5 death counts. The table also shows that the two-component Poisson model enjoys a significant improvement in the fit to the data.

<<proptable, results="asis">>=
tbl.prop = matrix(nrow = 3, ncol = 8)
tbl.prop[1,] = obs.prop*n
tbl.prop[2,] = round(exp.prop1*n,1)
tbl.prop[3,] = round(exp.prop2*n,1)
rownames(tbl.prop) = c("Observed",
                       "Expected 1-component",
                       "Expected 2-component")
colnames(tbl.prop) = c(0:6, "7+")

print(xtable(tbl.prop, auto=T, label="prop"))

@

<<rootogram, eval = F>>=
library(countreg)
rootogram(freq, fitted = )
@

\begin{thebibliography}{9}

\bibitem{navidi97}
  William Navidi,
  \emph{A graphical illustration of the EM algorithm},
  The American Statistician,
  Vol. 51, No. 1 (1997), 29-31


\bibitem{shapiro65}
  Samuel S.  Shapiro and Martin B. Wilk,
  \emph{An analysis of variance test 
  for normality (complete samples)},
  Biometrika, Vol. 52, No. 3/4 (1965), pp. 591--611

\end{thebibliography}

\end{document}